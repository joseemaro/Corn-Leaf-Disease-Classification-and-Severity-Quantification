{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import hashlib\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy import ndimage\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ Librer√≠as cargadas\")\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================\n",
    "DATA_PATH = 'color/color-img'        # Ruta a tu dataset\n",
    "OUTPUT_PATH = 'processed_data'       # Donde guardar datos procesados\n",
    "\n",
    "TARGET_SAMPLES_PER_CLASS = 1500  # MODIFICADO: 1500 im√°genes por clase\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "TRAIN_TEST_SPLIT = 0.8  # MODIFICADO: 80-20 split\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "print(f\"üìä Configuraci√≥n: {TARGET_SAMPLES_PER_CLASS} imgs/clase, {IMG_SIZE} size, {TRAIN_TEST_SPLIT:.0%} train split\")\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "CORN_CLASSES = {\n",
    "    0: 'Tiz√≥n foliar del norte (Northern Leaf Blight)',\n",
    "    1: 'Mancha gris de la hoja (Cercospora/Gray Leaf Spot)', \n",
    "    2: 'Roya com√∫n (Common Rust)',\n",
    "    3: 'Sano (Healthy)'\n",
    "}\n",
    "\n",
    "CORN_CLASSES_SHORT = {\n",
    "    0: 'Northern LB',\n",
    "    1: 'Gray LS', \n",
    "    2: 'Common Rust',\n",
    "    3: 'Healthy'\n",
    "}\n",
    "\n",
    "CORN_FOLDERS = [\n",
    "    'Corn_(maize)___Northern_Leaf_Blight',\n",
    "    'Corn_(maize)___Cercospora_leaf_spot',\n",
    "    'Corn_(maize)___Common_rust_',\n",
    "    'Corn_(maize)___healthy'\n",
    "]\n",
    "\n",
    "PREPROCESSING_CONFIG = {\n",
    "    'clahe_clip_limit': 2.0,\n",
    "    'clahe_tile_grid_size': (8, 8),\n",
    "    'amf_min_window': 3,\n",
    "    'amf_max_window': 7,\n",
    "}\n",
    "\n",
    "# Configuraci√≥n de augmentation para alcanzar 1500 im√°genes por clase\n",
    "AUGMENTATION_CONFIG = {\n",
    "    'rotation_range': 45,\n",
    "    'zoom_range': 0.2,\n",
    "    'horizontal_flip': True,\n",
    "    'vertical_flip': False,\n",
    "    'brightness_range': [1.1, 1.5],\n",
    "    'channel_shift_range': 0.1,\n",
    "    'width_shift_range': 0.2,\n",
    "    'height_shift_range': 0.2,\n",
    "    'shear_range': 0.2,\n",
    "    'fill_mode': 'nearest'\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# FUNCIONES DE EXPLORACI√ìN DEL DATASET\n",
    "# ============================================\n",
    "def get_all_image_files(folder_path):\n",
    "    \"\"\"Obtener todos los archivos de imagen\"\"\"\n",
    "    all_files = []\n",
    "    base_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "    \n",
    "    for ext in base_extensions:\n",
    "        files_lower = list(folder_path.glob(ext))\n",
    "        files_upper = list(folder_path.glob(ext.upper()))\n",
    "        all_files.extend(files_lower)\n",
    "        all_files.extend(files_upper)\n",
    "    \n",
    "    unique_files = list(set(str(f.resolve()) for f in all_files))\n",
    "    return [Path(f) for f in unique_files]\n",
    "\n",
    "def explore_dataset(data_path):\n",
    "    \"\"\"Explorar la estructura del dataset antes de cargar las im√°genes\"\"\"\n",
    "    print(\"üìÇ Explorando dataset...\")\n",
    "    \n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ùå Error: La ruta {data_path} no existe\")\n",
    "        return False, {}\n",
    "    \n",
    "    total_images = 0\n",
    "    class_distribution = {}\n",
    "    class_info = {}\n",
    "    \n",
    "    for idx, folder in enumerate(CORN_FOLDERS):\n",
    "        folder_path = data_path / folder\n",
    "        \n",
    "        if not folder_path.exists():\n",
    "            print(f\"‚ùå {CORN_CLASSES_SHORT[idx]}: Carpeta no encontrada\")\n",
    "            class_distribution[idx] = 0\n",
    "            class_info[idx] = {'path': str(folder_path), 'files': [], 'count': 0}\n",
    "            continue\n",
    "        \n",
    "        all_files = get_all_image_files(folder_path)\n",
    "        \n",
    "        class_distribution[idx] = len(all_files)\n",
    "        class_info[idx] = {\n",
    "            'path': str(folder_path),\n",
    "            'files': [str(f) for f in all_files],\n",
    "            'count': len(all_files)\n",
    "        }\n",
    "        total_images += len(all_files)\n",
    "        \n",
    "        print(f\"‚úÖ {CORN_CLASSES_SHORT[idx]}: {len(all_files)} im√°genes\")\n",
    "    \n",
    "    if total_images > 0:\n",
    "        print(f\"\\nüìä Total: {total_images} im√°genes\")\n",
    "        print(f\"üéØ Se generar√°n: {TARGET_SAMPLES_PER_CLASS * 4} im√°genes ({TARGET_SAMPLES_PER_CLASS} por clase)\")\n",
    "    \n",
    "    return total_images > 0, class_info\n",
    "\n",
    "# ============================================\n",
    "# PREPROCESAMIENTO HSV PARA ENFERMEDADES (NUEVO)\n",
    "# ============================================\n",
    "def apply_hsv_enhancement_for_disease(image):\n",
    "    \"\"\"\n",
    "    Aplicar mejora HSV solo para detectar mejor enfermedades\n",
    "    NO se aplica a im√°genes de clase Healthy\n",
    "    \"\"\"\n",
    "    # Convertir a HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    \n",
    "    # Mejorar saturaci√≥n para resaltar colores de enfermedades\n",
    "    # Las enfermedades suelen tener colores amarillentos, marrones o rojizos\n",
    "    s_enhanced = cv2.multiply(s, 1.1)  # Aumentar saturaci√≥n 30%\n",
    "    s_enhanced = np.clip(s_enhanced, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Ajustar value para mejorar contraste\n",
    "    clahe_v = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(6,6))\n",
    "    v_enhanced = clahe_v.apply(v)\n",
    "    \n",
    "    # Recombinar canales\n",
    "    hsv_enhanced = cv2.merge([h, s_enhanced, v_enhanced])\n",
    "    \n",
    "    # Convertir de vuelta a RGB\n",
    "    rgb_enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb_enhanced\n",
    "\n",
    "# ============================================\n",
    "# ADAPTIVE MEDIAN FILTER OPTIMIZADO\n",
    "# ============================================\n",
    "def adaptive_median_filter_fast(image, min_window=3, max_window=7):\n",
    "    \"\"\"\n",
    "    Versi√≥n optimizada del AMF usando operaciones vectorizadas\n",
    "    \"\"\"\n",
    "    result = np.copy(image).astype(np.float64)\n",
    "    rows, cols = image.shape\n",
    "    processed = np.zeros((rows, cols), dtype=bool)\n",
    "    \n",
    "    for window_size in range(min_window, max_window + 1, 2):\n",
    "        if np.all(processed):\n",
    "            break\n",
    "            \n",
    "        median_filtered = ndimage.median_filter(image, size=window_size)\n",
    "        min_filtered = ndimage.minimum_filter(image, size=window_size)\n",
    "        max_filtered = ndimage.maximum_filter(image, size=window_size)\n",
    "        \n",
    "        level_A_valid = (min_filtered < median_filtered) & (median_filtered < max_filtered)\n",
    "        level_B_valid = (min_filtered < image) & (image < max_filtered)\n",
    "        \n",
    "        keep_original = level_A_valid & level_B_valid & ~processed\n",
    "        result[keep_original] = image[keep_original]\n",
    "        processed[keep_original] = True\n",
    "        \n",
    "        use_median = level_A_valid & ~level_B_valid & ~processed\n",
    "        result[use_median] = median_filtered[use_median]\n",
    "        processed[use_median] = True\n",
    "    \n",
    "    if not np.all(processed):\n",
    "        final_median = ndimage.median_filter(image, size=max_window)\n",
    "        result[~processed] = final_median[~processed]\n",
    "    \n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "# ============================================\n",
    "# PIPELINE DE PREPROCESAMIENTO COMPLETO\n",
    "# ============================================\n",
    "def preprocess_image_pipeline(image_path, class_idx, visualize=False):\n",
    "    \"\"\"\n",
    "    Pipeline de preprocesamiento del paper + HSV para enfermedades:\n",
    "    RGB ‚Üí HSV enhancement (si es enfermedad) ‚Üí L*a*b* ‚Üí CLAHE ‚Üí AMF ‚Üí Resize ‚Üí Normalize\n",
    "    \n",
    "    Args:\n",
    "        image_path: Ruta de la imagen o array numpy\n",
    "        class_idx: √çndice de la clase (0-2: enfermedades, 3: healthy)\n",
    "        visualize: Si mostrar el proceso paso a paso\n",
    "    \n",
    "    Returns:\n",
    "        Imagen preprocesada normalizada (0-1) con shape (H, W, 3)\n",
    "    \"\"\"\n",
    "    # Leer imagen\n",
    "    if isinstance(image_path, str):\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        original = img.copy()\n",
    "    else:\n",
    "        img = image_path.copy()\n",
    "        original = img.copy()\n",
    "    \n",
    "    try:\n",
    "        # Convertir BGR a RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # NUEVO: Aplicar HSV enhancement \n",
    "        if class_idx == 1:  # solo gray ls\n",
    "            img_rgb = apply_hsv_enhancement_for_disease(img_rgb)\n",
    "        \n",
    "        # Pipeline completo del paper\n",
    "        # 1. Convertir RGB a LAB\n",
    "        lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n",
    "        l_channel, a_channel, b_channel = cv2.split(lab)\n",
    "        \n",
    "        # 2. Aplicar CLAHE al canal L\n",
    "        clahe = cv2.createCLAHE(\n",
    "            clipLimit=PREPROCESSING_CONFIG['clahe_clip_limit'],\n",
    "            tileGridSize=PREPROCESSING_CONFIG['clahe_tile_grid_size']\n",
    "        )\n",
    "        l_clahe = clahe.apply(l_channel)\n",
    "        \n",
    "        # 3. Aplicar AMF al canal L con CLAHE\n",
    "        l_amf = adaptive_median_filter_fast(\n",
    "            l_clahe, \n",
    "            min_window=PREPROCESSING_CONFIG['amf_min_window'], \n",
    "            max_window=PREPROCESSING_CONFIG['amf_max_window']\n",
    "        )\n",
    "        \n",
    "        # 4. Recombinar canales\n",
    "        lab_amf = cv2.merge([l_amf, a_channel, b_channel])\n",
    "        \n",
    "        # 5. Convertir de vuelta a RGB\n",
    "        img_filtered = cv2.cvtColor(lab_amf, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        # 6. Resize\n",
    "        img_resized = cv2.resize(img_filtered, IMG_SIZE)\n",
    "        \n",
    "        # 7. Normalizar\n",
    "        img_normalized = img_resized / 255.0\n",
    "        \n",
    "        # Verificar formato final\n",
    "        if len(img_normalized.shape) == 2:\n",
    "            img_normalized = np.stack([img_normalized] * 3, axis=-1)\n",
    "        elif img_normalized.shape[-1] != 3:\n",
    "            return None\n",
    "        \n",
    "        # Verificaci√≥n final\n",
    "        assert img_normalized.shape == (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "        assert 0 <= img_normalized.min() <= img_normalized.max() <= 1\n",
    "        \n",
    "        return img_normalized\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en preprocesamiento: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ============================================\n",
    "# GENERACI√ìN DE IM√ÅGENES SINT√âTICAS\n",
    "# ============================================\n",
    "def generate_synthetic_images(images, labels, target_per_class=1500):\n",
    "    \"\"\"\n",
    "    Genera im√°genes sint√©ticas usando augmentation para alcanzar target_per_class\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Generando im√°genes sint√©ticas para alcanzar {target_per_class} por clase...\")\n",
    "    \n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    # Crear generador de augmentation\n",
    "    datagen = ImageDataGenerator(**AUGMENTATION_CONFIG)\n",
    "    \n",
    "    for class_idx in range(4):\n",
    "        class_images = images[labels == class_idx]\n",
    "        current_count = len(class_images)\n",
    "        needed = target_per_class - current_count\n",
    "        \n",
    "        print(f\"\\n{CORN_CLASSES_SHORT[class_idx]}:\")\n",
    "        print(f\"  ‚Ä¢ Originales: {current_count}\")\n",
    "        print(f\"  ‚Ä¢ Necesarias: {needed}\")\n",
    "        \n",
    "        # Agregar todas las im√°genes originales\n",
    "        augmented_images.extend(class_images)\n",
    "        augmented_labels.extend([class_idx] * current_count)\n",
    "        \n",
    "        if needed > 0:\n",
    "            # Generar im√°genes sint√©ticas\n",
    "            generated_count = 0\n",
    "            iterations = 0\n",
    "            max_iterations = needed * 5  # Prevenir bucle infinito\n",
    "            \n",
    "            while generated_count < needed and iterations < max_iterations:\n",
    "                # Seleccionar imagen aleatoria de la clase\n",
    "                idx = np.random.randint(0, current_count)\n",
    "                img = class_images[idx:idx+1]\n",
    "                \n",
    "                # Generar una versi√≥n aumentada\n",
    "                for batch in datagen.flow(img, batch_size=1):\n",
    "                    augmented_img = batch[0]\n",
    "                    augmented_images.append(augmented_img)\n",
    "                    augmented_labels.append(class_idx)\n",
    "                    generated_count += 1\n",
    "                    iterations += 1\n",
    "                    break\n",
    "                \n",
    "                if generated_count % 100 == 0 and generated_count > 0:\n",
    "                    print(f\"    Generadas: {generated_count}/{needed}\")\n",
    "            \n",
    "            print(f\"  ‚úÖ Total generadas: {generated_count}\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ No se necesitan im√°genes adicionales\")\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    augmented_images = np.array(augmented_images, dtype=np.float32)\n",
    "    augmented_labels = np.array(augmented_labels, dtype=np.int32)\n",
    "    \n",
    "    # Mezclar los datos\n",
    "    indices = np.random.permutation(len(augmented_images))\n",
    "    augmented_images = augmented_images[indices]\n",
    "    augmented_labels = augmented_labels[indices]\n",
    "    \n",
    "    print(f\"\\nüìä Total final: {len(augmented_images)} im√°genes\")\n",
    "    \n",
    "    return augmented_images, augmented_labels\n",
    "\n",
    "# ============================================\n",
    "# CARGA Y PROCESAMIENTO DEL DATASET\n",
    "# ============================================\n",
    "def load_and_preprocess_dataset(data_path, show_progress=True):\n",
    "    \"\"\"\n",
    "    Cargar y preprocesar im√°genes del dataset aplicando CLAHE+AMF+HSV\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    file_paths = []\n",
    "    processing_log = []\n",
    "    \n",
    "    print(\"üîß Cargando y preprocesando con CLAHE + AMF + HSV enhancement...\")\n",
    "    \n",
    "    total_loaded = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for idx, folder in enumerate(CORN_FOLDERS):\n",
    "        folder_path = Path(data_path) / folder\n",
    "        class_name = CORN_CLASSES_SHORT[idx]\n",
    "        \n",
    "        print(f\"\\n{class_name}:\")\n",
    "        \n",
    "        all_imgs = get_all_image_files(folder_path)\n",
    "        \n",
    "        if not all_imgs:\n",
    "            print(f\"  ‚ö†Ô∏è No se encontraron im√°genes\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Disponibles: {len(all_imgs)} im√°genes\")\n",
    "        \n",
    "        class_loaded = 0\n",
    "        class_errors = 0\n",
    "        \n",
    "        for img_path in all_imgs:\n",
    "            try:\n",
    "                # Preprocesar imagen con √≠ndice de clase para HSV enhancement\n",
    "                processed_img = preprocess_image_pipeline(str(img_path), idx, visualize=False)\n",
    "                \n",
    "                if processed_img is not None:\n",
    "                    images.append(processed_img)\n",
    "                    labels.append(idx)\n",
    "                    file_paths.append(str(img_path))\n",
    "                    class_loaded += 1\n",
    "                    total_loaded += 1\n",
    "                    \n",
    "                    if show_progress and total_loaded % 100 == 0:\n",
    "                        print(f\"    Procesadas: {total_loaded}\")\n",
    "                        \n",
    "                else:\n",
    "                    class_errors += 1\n",
    "                    total_errors += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Error en {img_path.name}: {str(e)}\")\n",
    "                class_errors += 1\n",
    "                total_errors += 1\n",
    "        \n",
    "        print(f\"  ‚úÖ Cargadas: {class_loaded}, Errores: {class_errors}\")\n",
    "        \n",
    "        processing_log.append({\n",
    "            'class': class_name,\n",
    "            'loaded': class_loaded,\n",
    "            'errors': class_errors,\n",
    "            'success_rate': class_loaded / (class_loaded + class_errors) if (class_loaded + class_errors) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        raise ValueError(\"No se pudieron cargar im√°genes v√°lidas\")\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    X = np.array(images, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DATASET CARGADO:\")\n",
    "    print(f\"  üìä Total im√°genes: {len(X)}\")\n",
    "    print(f\"  üìê Shape: {X.shape}\")\n",
    "    print(f\"  üéØ Clases: {len(np.unique(y))}\")\n",
    "    print(f\"  ‚ùå Errores: {total_errors}\")\n",
    "    \n",
    "    # Mostrar distribuci√≥n por clase\n",
    "    print(f\"\\nüìä DISTRIBUCI√ìN ORIGINAL POR CLASE:\")\n",
    "    for idx, class_name in CORN_CLASSES_SHORT.items():\n",
    "        count = np.sum(y == idx)\n",
    "        percentage = (count / len(y)) * 100 if len(y) > 0 else 0\n",
    "        print(f\"  ‚Ä¢ {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Guardar log de procesamiento\n",
    "    log_df = pd.DataFrame(processing_log)\n",
    "    log_df.to_csv(Path(OUTPUT_PATH) / 'preprocessing_log.csv', index=False)\n",
    "    \n",
    "    return X, y, file_paths, processing_log\n",
    "\n",
    "# ============================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ============================================\n",
    "def complete_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Pipeline completo con generaci√≥n de im√°genes sint√©ticas y divisi√≥n 80-20\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PIPELINE DE PREPROCESAMIENTO v3\")\n",
    "    print(\"CLAHE + AMF + HSV Enhancement + Synthetic Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Paso 1: Explorar dataset\n",
    "    print(\"\\nPASO 1: Exploraci√≥n del dataset\")\n",
    "    dataset_ok, dataset_info = explore_dataset(DATA_PATH)\n",
    "    \n",
    "    if not dataset_ok:\n",
    "        print(\"Error: Dataset no v√°lido\")\n",
    "        return None\n",
    "    \n",
    "    # Paso 2: Cargar y preprocesar im√°genes originales\n",
    "    print(\"\\nPASO 2: Carga y preprocesamiento\")\n",
    "    X_original, y_original, file_paths, processing_log = load_and_preprocess_dataset(\n",
    "        DATA_PATH, show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Paso 3: Generar im√°genes sint√©ticas para alcanzar 1500 por clase\n",
    "    print(\"\\nPASO 3: Generaci√≥n de im√°genes sint√©ticas\")\n",
    "    X_augmented, y_augmented = generate_synthetic_images(\n",
    "        X_original, y_original, target_per_class=TARGET_SAMPLES_PER_CLASS\n",
    "    )\n",
    "    \n",
    "    # Paso 4: Divisi√≥n estratificada 80-20\n",
    "    print(\"\\nPASO 4: Divisi√≥n estratificada 80-20\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_augmented, y_augmented, \n",
    "        test_size=(1 - TRAIN_TEST_SPLIT),\n",
    "        random_state=42,\n",
    "        stratify=y_augmented\n",
    "    )\n",
    "    \n",
    "    # Convertir a one-hot\n",
    "    y_train_cat = to_categorical(y_train, num_classes=4)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=4)\n",
    "    \n",
    "    print(f\"\\nDivisi√≥n final:\")\n",
    "    print(f\"  ‚Ä¢ Entrenamiento: {len(X_train)} im√°genes\")\n",
    "    print(f\"  ‚Ä¢ Test: {len(X_test)} im√°genes\")\n",
    "    \n",
    "    # Mostrar distribuci√≥n\n",
    "    print(\"\\nDISTRIBUCI√ìN POR CLASE:\")\n",
    "    for split_name, y_data in [(\"Train\", y_train), (\"Test\", y_test)]:\n",
    "        print(f\"\\n{split_name}:\")\n",
    "        unique, counts = np.unique(y_data, return_counts=True)\n",
    "        for cls, count in zip(unique, counts):\n",
    "            percentage = (count / len(y_data)) * 100\n",
    "            print(f\"  ‚Ä¢ {CORN_CLASSES_SHORT[cls]}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Preparar datos para guardar\n",
    "    processed_data = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train_cat,\n",
    "        'y_test': y_test_cat,\n",
    "        'y_train_raw': y_train,\n",
    "        'y_test_raw': y_test,\n",
    "        'class_names': CORN_CLASSES_SHORT,\n",
    "        'num_classes': 4,\n",
    "        'file_paths': file_paths,\n",
    "        'processing_info': {\n",
    "            'method': 'v3_clahe_amf_hsv_synthetic',\n",
    "            'preprocessing': 'CLAHE + AMF + HSV Enhancement for diseases',\n",
    "            'synthetic_generation': f'{TARGET_SAMPLES_PER_CLASS} per class',\n",
    "            'original_samples': len(X_original),\n",
    "            'augmented_samples': len(X_augmented),\n",
    "            'train_samples': len(X_train),\n",
    "            'test_samples': len(X_test),\n",
    "            'train_test_split': f'{TRAIN_TEST_SPLIT:.0%}-{1-TRAIN_TEST_SPLIT:.0%}',\n",
    "            'processing_date': datetime.now().isoformat(),\n",
    "            'augmentation_config': AUGMENTATION_CONFIG,\n",
    "            'preprocessing_config': PREPROCESSING_CONFIG\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Guardar datos\n",
    "    save_path = Path(OUTPUT_PATH)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Guardar arrays numpy\n",
    "    np.savez_compressed(\n",
    "        save_path / 'preprocessed_data_v3.npz',\n",
    "        **{k: v for k, v in processed_data.items() if isinstance(v, np.ndarray)}\n",
    "    )\n",
    "    \n",
    "    # Guardar metadata\n",
    "    with open(save_path / 'metadata_v3.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'class_names': CORN_CLASSES_SHORT,\n",
    "            'num_classes': 4,\n",
    "            'file_paths': file_paths,\n",
    "            'processing_info': processed_data['processing_info'],\n",
    "            'processing_log': processing_log,\n",
    "            'data_shapes': {k: v.shape for k, v in processed_data.items() if isinstance(v, np.ndarray)},\n",
    "            'CORN_CLASSES': CORN_CLASSES,\n",
    "            'CORN_CLASSES_SHORT': CORN_CLASSES_SHORT,\n",
    "            'AUGMENTATION_CONFIG': AUGMENTATION_CONFIG,\n",
    "            'PREPROCESSING_CONFIG': PREPROCESSING_CONFIG\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datos guardados en:\")\n",
    "    print(f\"  ‚Ä¢ {save_path}/preprocessed_data_v3.npz\")\n",
    "    print(f\"  ‚Ä¢ {save_path}/metadata_v3.pkl\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# ============================================\n",
    "# FUNCIONES DE VISUALIZACI√ìN\n",
    "# ============================================\n",
    "def visualize_preprocessing_comparison():\n",
    "    \"\"\"\n",
    "    Visualizar comparaci√≥n de preprocesamiento con y sin HSV enhancement\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Generando comparaci√≥n visual...\")\n",
    "    \n",
    "    # Cargar una imagen de cada clase\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "    \n",
    "    for idx, folder in enumerate(CORN_FOLDERS):\n",
    "        folder_path = Path(DATA_PATH) / folder\n",
    "        imgs = get_all_image_files(folder_path)\n",
    "        \n",
    "        if imgs:\n",
    "            # Leer imagen original\n",
    "            img = cv2.imread(str(imgs[0]))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Original\n",
    "            axes[idx, 0].imshow(img_rgb)\n",
    "            axes[idx, 0].set_title(f'{CORN_CLASSES_SHORT[idx]} - Original')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            # Con preprocesamiento est√°ndar\n",
    "            processed_standard = preprocess_image_pipeline(str(imgs[0]), 3, visualize=False)  # Como si fuera healthy\n",
    "            if processed_standard is not None:\n",
    "                axes[idx, 1].imshow(processed_standard)\n",
    "                axes[idx, 1].set_title('CLAHE + AMF')\n",
    "                axes[idx, 1].axis('off')\n",
    "            \n",
    "            # Con HSV enhancement (solo para enfermedades)\n",
    "            processed_hsv = preprocess_image_pipeline(str(imgs[0]), idx, visualize=False)\n",
    "            if processed_hsv is not None:\n",
    "                axes[idx, 2].imshow(processed_hsv)\n",
    "                title = 'CLAHE + AMF + HSV' if idx < 3 else 'CLAHE + AMF (sin HSV)'\n",
    "                axes[idx, 2].set_title(title)\n",
    "                axes[idx, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n de Preprocesamiento', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(OUTPUT_PATH) / 'preprocessing_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Visualizaci√≥n guardada en processed_data/preprocessing_comparison.png\")\n",
    "\n",
    "def verify_saved_data():\n",
    "    \"\"\"\n",
    "    Verificar y mostrar informaci√≥n sobre los datos guardados\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Verificando datos guardados...\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    data_path = Path(OUTPUT_PATH) / 'preprocessed_data_v3.npz'\n",
    "    metadata_path = Path(OUTPUT_PATH) / 'metadata_v3.pkl'\n",
    "    \n",
    "    if data_path.exists() and metadata_path.exists():\n",
    "        data = np.load(data_path)\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        print(\"\\n‚úÖ Datos cargados correctamente:\")\n",
    "        print(f\"  ‚Ä¢ X_train shape: {data['X_train'].shape}\")\n",
    "        print(f\"  ‚Ä¢ X_test shape: {data['X_test'].shape}\")\n",
    "        print(f\"  ‚Ä¢ y_train shape: {data['y_train'].shape}\")\n",
    "        print(f\"  ‚Ä¢ y_test shape: {data['y_test'].shape}\")\n",
    "        \n",
    "        # Mostrar algunas im√°genes de entrenamiento\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        for i in range(8):\n",
    "            if i < len(data['X_train']):\n",
    "                axes[i//4, i%4].imshow(data['X_train'][i])\n",
    "                class_idx = np.argmax(data['y_train'][i])\n",
    "                axes[i//4, i%4].set_title(metadata['class_names'][class_idx])\n",
    "                axes[i//4, i%4].axis('off')\n",
    "        \n",
    "        plt.suptitle('Muestras del dataset procesado', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se encontraron datos guardados. Ejecuta el pipeline primero.\")\n",
    "\n",
    "# ============================================\n",
    "# EJECUCI√ìN PRINCIPAL\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar pipeline completo\n",
    "    result = complete_preprocessing_pipeline()\n",
    "    \n",
    "    # Generar visualizaci√≥n comparativa\n",
    "    if result is not None:\n",
    "        visualize_preprocessing_comparison()\n",
    "        verify_saved_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
